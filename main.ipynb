{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep fake detection challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup tensorflow to use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "gpu = False\n",
    "# Use gpu if available\n",
    "if gpu:\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    print(os.environ['CUDA_VISIBLE_DEVICES'])  # Check the value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the seed for random number generators to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Procesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "- Load the data from the directory\n",
    "- Extract 20 random frames from each video\n",
    "- Use MTcnn to detect faces in the frames. If no face is detected, discard the frame. If more than one face is detected, take the first face since both faces have been faked similarly.\n",
    "- Resize the frames to 224x224\n",
    "- Save the frames in a new directory and create a new image_label.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "# Paths\n",
    "data_folder = \"data\"\n",
    "output_folder = \"data_images\"\n",
    "\n",
    "# If the output folder is not empty, skip processing\n",
    "if os.path.exists(output_folder) and len(os.listdir(output_folder)) > 0:\n",
    "    print(\"Skipping processing: 'data_images' folder is already populated.\")\n",
    "else:\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Initialize MTCNN\n",
    "    detector = MTCNN()\n",
    "\n",
    "    # Parameters\n",
    "    num_frames = 20\n",
    "    frame_size = (224, 224)\n",
    "\n",
    "    # Load labels from the existing metadata\n",
    "    with open(\"data/metadata.json\", \"r\") as f:\n",
    "        video_labels = json.load(f)\n",
    "\n",
    "    # Dictionary to store image-label mappings\n",
    "    image_labels = {}\n",
    "\n",
    "    # Process videos\n",
    "    for video_name in os.listdir(data_folder):\n",
    "        video_path = os.path.join(data_folder, video_name)\n",
    "\n",
    "        if video_name.endswith(\".mp4\"):\n",
    "            reader = imageio.get_reader(video_path, \"ffmpeg\")\n",
    "            total_frames = reader.count_frames()\n",
    "\n",
    "            # Select frame indices\n",
    "            selected_frame_indices = sorted(random.sample(range(total_frames), num_frames))\n",
    "\n",
    "            # Extract frames and process\n",
    "            for i, frame in enumerate(reader):\n",
    "                if i in selected_frame_indices:\n",
    "                    # Detect faces in the frame and choose the first face\n",
    "                    faces = detector.detect_faces(frame)\n",
    "                    \n",
    "                    # If no faces are found, skip this frame\n",
    "                    if len(faces) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Get the bounding box of the first face\n",
    "                    face = faces[0]\n",
    "                    x, y, w, h = face['box']\n",
    "\n",
    "                    # Crop the face region\n",
    "                    face_crop = frame[y:y+h, x:x+w]\n",
    "                    \n",
    "                    if face_crop.size == 0:  # Ensure the cropped face is valid\n",
    "                        continue\n",
    "                    \n",
    "                    # Resize the cropped face to 224x224\n",
    "                    face_resized = cv2.resize(face_crop, frame_size)\n",
    "\n",
    "                    # Generate a unique image name for the face\n",
    "                    image_name = f\"{video_name.split('.')[0]}_{i}.jpg\"\n",
    "                    image_path = os.path.join(output_folder, image_name)\n",
    "\n",
    "                    # Save the resized face image\n",
    "                    cv2.imwrite(image_path, face_resized)\n",
    "\n",
    "                    # Store the label mapping\n",
    "                    video_metadata = video_labels.get(video_name)\n",
    "                    if video_metadata:\n",
    "                        image_labels[image_name] = video_metadata[\"label\"]\n",
    "\n",
    "            reader.close()\n",
    "\n",
    "    # Save the label mappings to a JSON file\n",
    "    with open(\"image_labels.json\", \"w\") as f:\n",
    "        json.dump(image_labels, f, indent=4)\n",
    "\n",
    "    print(\"Processing complete. Images saved in 'data_images', labels in 'image_labels.json'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Label Counts Plot\n",
    "- The plot above visualizes the count of `REAL` and `FAKE` images extracted from the videos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_labels = json.load(open(\"image_labels.json\", \"r\"))\n",
    "# Count the number of REAL and FAKE images outside of the loop\n",
    "real_images_count = sum(1 for label in image_labels.values() if label == \"REAL\")\n",
    "fake_images_count = sum(1 for label in image_labels.values() if label == \"FAKE\")\n",
    "\n",
    "# Plotting the results\n",
    "labels = ['REAL', 'FAKE']\n",
    "counts = [real_images_count, fake_images_count]\n",
    "\n",
    "plt.bar(labels, counts, color=['green', 'red'])\n",
    "plt.xlabel('Image Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('REAL vs FAKE Image Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the images and labels and set it to a tf.data.Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the image-label mappings\n",
    "image_labels = json.load(open(\"image_labels.json\", \"r\"))\n",
    "\n",
    "fakes = np.empty((0, 2), dtype=object)\n",
    "reals = np.empty((0, 2), dtype=object)\n",
    "\n",
    "for image, label in image_labels.items():\n",
    "    if label == \"FAKE\":\n",
    "        fakes = np.append(fakes, np.array([[image, label]]), axis=0)\n",
    "    else:\n",
    "        reals = np.append(reals, np.array([[image, label]]), axis=0)\n",
    "        \n",
    "data = np.vstack((fakes, reals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perorm train-test split while stratifying on the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42, stratify=data[:, 1])\n",
    "\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42, stratify=train[:, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the total number of images in each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "unique_labels, label_counts = np.unique(train[:, 1], return_counts=True)\n",
    "axes[0].bar(unique_labels, label_counts, edgecolor='black')\n",
    "axes[0].set_xticks(unique_labels)\n",
    "axes[0].set_xticklabels(['Fakes', 'Real'])\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Number of Images')\n",
    "axes[0].set_title('Number of Images per Label in Training Set')\n",
    "\n",
    "unique_labels, label_counts = np.unique(val[:, 1], return_counts=True)\n",
    "axes[1].bar(unique_labels, label_counts, edgecolor='black')\n",
    "axes[1].set_xticks(unique_labels)\n",
    "axes[0].set_xticklabels(['Fakes', 'Real'])\n",
    "axes[1].set_xlabel('Label')\n",
    "axes[1].set_ylabel('Number of Images')\n",
    "axes[1].set_title('Number of Images per Label in Validation Set')\n",
    "\n",
    "unique_labels, label_counts = np.unique(test[:, 1], return_counts=True)\n",
    "axes[2].bar(unique_labels, label_counts, edgecolor='black')\n",
    "axes[2].set_xticks(unique_labels)\n",
    "axes[0].set_xticklabels(['Fakes', 'Real'])\n",
    "axes[2].set_xlabel('Label')\n",
    "axes[2].set_ylabel('Number of Images')\n",
    "axes[2].set_title('Number of Images per Label in Test Set')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the images under train, test and val folders with the respective labels in the folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def save_images(data, base_folder):\n",
    "    if not os.path.exists(base_folder):\n",
    "        os.makedirs(base_folder)\n",
    "    for img_path, label in data:\n",
    "        label_folder = os.path.join(base_folder, str(label))\n",
    "        if not os.path.exists(label_folder):\n",
    "            os.makedirs(label_folder)\n",
    "        img = Image.open(img_path)\n",
    "        img.save(os.path.join(label_folder, os.path.basename(img_path)))\n",
    "\n",
    "if os.path.exists('train') and os.path.exists('val') and os.path.exists('test') and len(os.listdir(output_folder)) > 0:\n",
    "    print(\"Skipping processing: 'train, val, test' folder is already populated.\")\n",
    "else:\n",
    "    # Save the images for each dataset\n",
    "    save_images(train, 'train')\n",
    "    save_images(val, 'val')\n",
    "    save_images(test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the images from the folders and perform data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create ImageDataGenerators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,              # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,           # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.2,       # Randomly shift images horizontally by 20% of the width\n",
    "    height_shift_range=0.2,      # Randomly shift images vertically by 20% of the height\n",
    "    shear_range=0.2,             # Apply shearing transformations\n",
    "    zoom_range=0.2,              # Randomly zoom in or out by 20%\n",
    "    horizontal_flip=True,        # Randomly flip images horizontally\n",
    "    fill_mode='nearest'          # Fill missing pixels after transformations\n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'train',                   \n",
    "    target_size=(224, 224),     \n",
    "    batch_size=32,              \n",
    "    class_mode='categorical'    \n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    'val', \n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'test', \n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
